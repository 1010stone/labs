---
layout: page
title: Multiple testing
---

```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

```{r}
library(rafalib)
```

## Procedures

In the previous section we learned how p-values are no longer
a useful quantity to use as a cut-off when you're looking at high-dimensional data and you are testing many units at the same time. This is referred to as the _multiple comparison_ or _multiple testing_ or _multiplicity_ problem. The definition of a p-value does not provide a useful quantification here. Again, because when we test many hypotheses simultaneusly, a list just based on a small p-values cut-off of, say 0.01, can result in many false positives. Here we define terms that are more appropriat

The most widely used approach to the multiplicity problem is to define a _procedure_ and the estimate an informative _error rate_ for this procedure. The procuedures are typically flexible through parameters or cutoffs that let us control specificity and sensitivity. An example of a procedure is: 

* Compute a p-value for each gene
* Call signficant all genes with p-values smaller than $\alpha$

Note that changing the $\alpha$ permits us to adjust specificity and sensitivity. 

Next we define the _error rates_  that we will try to estimate and control.

## Error Rates

Throughout this section we will be using the type I error and a type II error terminology. Note that in the context of high-throughput data we can have several type I erros and several type II erros, as opposed to one or the other. In this table we summarize the possibilities using the notation from the seminal paper by Benjamini-Hochberg:

|   | Called significant   | Not called significant   | Total  |  
|---|---|---|---|
|Null True   |$V$   | $m_0-V$  | $m_0$  |
|Alternative True   | $S$  | $m_1-S$    | $m_1$    |   
|True   | $R$  |  $m-R$ | $m$  |

To describe the entries in the table let's use as an example an dataset with 20,000 genes which means that the total number of tests that we are conducting is: $m=20,000$. The number of genes for which the null hypothesis is true, representing the genes we are not intereted in, is $m_0$ while the number of genes for which the null hypothesis is not ture is $m_1$ For most high-throughput experiments, we assume that $m_0$ is much greater than $m_1$. You test 20,000 and maybe 10 are of interest, so $m_1=10$ and 
and $m_0=19,990$. We have several genes and $R$ represents the total number of genes that whatever procedure we are evaulating calls significant. $m-R$ is therefore is the total number of genes we don't call significant. The rest of the table contains important quantities that are unknown. 

* $V$ represents the number of genes
that we call significant where the null hypothesis is true. So $V$ is the total number of type I errors. 
* $S$ represents the number of genes that are
called significant where the alternative is true also called _true positives_.

This implies that there are $m_1-S$ type II errors or _false negatives_ and $m_0-V$ true negatives. 

Note that if we only ran one test,
a p-value is simply the probability that $V=1$. Power is the probability of $S=1$. In this very simple case, we wouldn't bother making tables. Below we will how defining the terms in the table helps in the high-dimensional context.

  
### Data Example

Let's compute these quantities with a data example. We will use a Monte Carlo simulation using our mice data to immitate a situation in which we perform tests for 1000 diets, all of them having no effects. The null hypothesis is true for all of them thus $m=100, m_0=100,$ and $m_1=0$. Let's run the tests with a sample size of $N=12$ and compute $R$. Our procedure will declare any diet achieving a p-value smaller than $\alpha=0.05$ as significant. 


```{r}
set.seed(1)
population = unlist( read.csv("femaleControlsPopulation.csv") )
alpha <- 0.05
N <- 12
m <- 1000
calls <- replicate(m,{
  control = sample(population,N)
  treatment = sample(population,N)
  t.test(treatment,control)$p.val < alpha
})
```

Although in practice we do not know the truth, in this simulation we do, so we can actually compute $V$ and $S$. Because all null hypotheses are true, we know $V=R$, a 

```{r}
sum(calls) ##This is R
```
These many false positives is not acceptable in most contexts.

Here is more complicated code showing results where 10% of the diets are effective with an average effect size of $\Delta=0.5$ ounce. 
Studying this code carefully will help undertand the meaning of the table above.

```{r}
alpha <- 0.05
N <- 12
m <- 1000
nullHypothesis <- c( rep(TRUE,900), rep(FALSE,100))
delta <- 1
calls <- sapply(1:m, function(i){
  control <- sample(population,N)
  treatment <- sample(population,N)
  if(!nullHypothesis[i]) treatment <- treatment + delta
  ifelse( t.test(treatment,control)$p.val < alpha, "Called Significant","Not Called Significant")
})
nullHypothesis <- factor( nullHypothesis, levels=c("TRUE","FALSE"))
table(nullHypothesis,calls)
```

The first column of the table above shows us $V$ and $S$.

Note that $V$ and $S$ are random variables. If we run the simulation over and over again, these values change. Here is a quick example:
```{r}
alpha <- 0.05
N <- 12
m <- 1000
nullHypothesis <- c( rep(TRUE,900), rep(FALSE,100))
delta <- 1
B <- 10 ## number of simulations
VandS <- replicate(B,{
  calls <- sapply(1:m, function(i){
    control <- sample(population,N)
    treatment <- sample(population,N)
    if(!nullHypothesis[i]) treatment <- treatment + delta
    t.test(treatment,control)$p.val < alpha
  })
  cat("V =",sum(nullHypothesis & calls), "S =",sum(!nullHypothesis & calls),"\n")
  c(sum(nullHypothesis & calls),sum(!nullHypothesis & calls))
  })
```

This motivates the definition of error rates. We can, for example, estimate probability that $V$ is larger than 0. This is intepreted as the probability of making at least one type I error among the 1000 tests. In the example we made many more than 1 in every single simulation so we suspect this probability is very close to 1. When $m=1$, this probability is equivalent to the p-value. When we have a multiple tests situation, we call it the Family Wide Error Rate (FWER) and it relates to technique that is widely used: The Bonferroni correction.


## The Sidak's procedure

In the previous section we saw how the probability of incorrectly rejecting the null for at least one of the 1,000 null experiments was well over 0.05. We can actually compute that probability:

Let $p_1,\dots,p_{1000}$ be the the p-values (random variables). Then 

$$
\begin{align*}
\mbox{Pr}(\mbox{at least one rejection}) &= 1 -\mbox{Pr}(\mbox{no rejections}) \\
&= 1 - \prod_{i=1}^{1000} \mbox{Pr}(p_i>0.05) \\
&= 1-0.95^{1000} \approx 1
\end{align*}
$$

Or if you want to use a simulations:

```{r}
B<-1000
minpval <- replicate(B, min(runif(10000,0,1))<0.05)
mean(minpval>=1)
```


So what do we need to do to make the probability of a mistake very small, say 5%? Using the derivation above we can  change the procedure by selecting a more sringent cutoff, previously 0.05, to lower our probability of at least one mistake to be 5%. Namely, by noting that 

$$\mbox{Pr}(\mbox{at least one rejection}) =  1-(1-k)^{1000}$$

and solving for $k$ we get $1-(1-k)^{100}=0.05 \implies k = 1-0.95^{1/100} \approx 0.0005$

These derivation and simulation should help us undertand what we meant by procedures above and what it means to control error rates.  You can think of it as defnining a set of instructions, such as "reject all the null hypothesis for  for which p-values < 0.0005". Then, knowing the p-values are random variables, we use statistical theory to compute how many mistakes, on average, will we make if we follow this procedure. More precisely we compute bounds on these rates, meaning that we show that they are smaller than some predermined value (there is a preference in the life sciences to err on the side of being conservative)


## Bonferonni correction

So we have learned about the family wide error rate FWER. This is the probability of incorrectly rejecting the null at least once. Using the notation in the video this probability is written like this: $\mbox{Pr}(V>0)$. 

What we want to do in practice is choose a _procedure_ that guarantees this probability is smaller than a predetermined value such as 0.05. Here we keep it general and instead of 0.05 we use $\alpha$. We have already learned that the procedure "reject all the hypotheses with p-value <0.05" fails  miserably as we have seen that $Pr(V>0) \approx 1$. So what else can we do?

The Bonferroni procedure assumes we have computed p-values for each test and asks what constant $k$ should we pick so that the procedure "reject all hypotheses with p-value less than $k$" has $\mbox{Pr}(V>0) = 0.05$. And we typically want to be conservative rather than lenient, so we accept a procedure that has $\mbox{Pr}(V>0) \leq 0.05$. 

So the first result we rely on is that this probability is largest when all the null hypotheses are true:

$$\mbox{Pr}(V>0) \leq \mbox{Pr}(V>0 \mid \mbox{all nulls are true})$$

or using the notation from the table above:

$$\mbox{Pr}(V>0) \leq \mbox{Pr}(V>0 \mid m_1=0)$$

Because these tests are independent we know that :

$$\mbox{Pr}(V>0 \mid m_1=0) = 1-(1-k)^m$$

And we pick $k$ so that $1-(1-k)^m = \alpha \implies k = 1-(1-\alpha)^{1/m}$. This cutoff defines  Sidak's procedure.

Now, this requires the tests to be independent. The Bonferroni procedure does not make this assumption. Instead we  set $k=\alpha/m$ and shows that with this for this procedure $Pr(V>0) \leq \alpha$:

$$
\begin{align*}
\mbox{Pr}(V>0 \,\mid \, m_1=0) &= \mbox{Pr}\left( \min_i \{p_i\} \leq \frac{\alpha}{m} \mid m_1=0 \right)\\
 &\leq \sum_{i=1}^m\leq \left(p_i \leq \frac{\alpha}{m} \right)\\
 &= m \frac{\alpha}{m}=\alpha
\end{align*}
$$

Monte Carlo simulation. To simulate the p-value results of, say a 8,792 t-tests for which the null is true we don't actual have to generate the original data. As we learned in class we can generate p-values for a uniform distribution like this:

```{r}
pvals <- runif(1000,0,1)
```

Using what we have learned set the cutoff using the Bonferroni correction and report back the FWER. Set the seed at 1 and run 10,000 simulation.


```{r}
set.seed(1)
B <- 10000
m <- 1000
alpha <- 0.05
pvals <- matrix(runif(B*m,0,1),B,m)

##Bonferroni
k <- alpha/m
mistakes <- rowSums(pvals<k) 
mean(mistakes>0)

##Sidak's
k <- (1-(1-alpha)^(1/m))
mistakes <- rowSums(pvals<k) 
mean(mistakes>0)
```


## False Discovery Rate 

There are many situations for which requiring an FWER of 0.05 does not make sense as it is much to strict. For example, consider the very common exercise of running a preliminary small study to determine a handful of candidate genes. This is referred to as a _discovery_ driven project or experiment. We may be in search of an unknown causative gene and more than willing to perform follow up studies with many more samples on just the candidates. If we develop a procedure that produces, for example, a list of 10 genes of which 1 or 2 pan out as important, the experiment is a resounding success. Note that with a small sample size, the only way to achiev a FWER $\leq$ 0.05 is with an empty list of genes. 

Note that by requiring a FWER $\leq$ 0.05 we are practically assuring 0 power (sensitivy) and that the specifcity reqruiement is over-kill. A widely used alternative to the FWER is the false discover rate (FDR). The idea behind FDR is to cosider the random variable $Q \equiv V/R$ with $Q=0$ when $R=0$ and $V=0$. Note that $R=0$ (nothing called significant) implies $V=0$ (no false positives). So $Q$ is a random variable that can take values between 0 and 1 and we can define a rate by considering the average of $Q$. To better understand this concept. Here we compute $Q$ for the procedure: call everything p-value < 0.05 significant.


```{r}
alpha <- 0.05
N <- 12
m <- 100
m1 <- m*.9
m0 <- m*.1
nullHypothesis <- c( rep(TRUE,m0), rep(FALSE,m1))
delta <- 1
B <- 1000 ## number of simulations
Qs <- replicate(B,{
  calls <- sapply(1:m, function(i){
    control <- sample(population,N)
    treatment <- sample(population,N)
    if(!nullHypothesis[i]) treatment <- treatment + delta
    t.test(treatment,control)$p.val < alpha
  })
 R=sum(calls)
 Q=ifelse(R>0,sum(nullHypothesis & calls)/R,0)
 return(Q)
})
mypar(1,1)
hist(Qs) ##Q is a random variable, this is its distribution
FDR=mean(Qs)
print(FDR)
```

The FDR is relatively high here. This is because for 90% of the tests the null hypotheses is true. Thi implies that with a 0.05 p-value cut-off, out of the 100 tests we incorrectly call between 4 and 5 significant on average. This combined with the fact that we don't "catch" all the cases where the alternative is true, gives us a relatively high FDR. So how can we control this? What if we want lower FDR, say 25%.

To visually see why the FDR is high, we can make a histogram of the p-values. We use a higher value of `m` to have more data from the histogram. We draw a horizontal line representing the uniform distribution one gets for the `m0` cases for which the null is true. 


```{r}
m <- 10000
m0 <- m*.9 
m1 <- m-m0
nullHypothesis <- c( rep(TRUE,m0), rep(FALSE,m1))
delta <- 3
pvals <- sapply(1:m, function(i){
    control <- sample(population,N)
    treatment <- sample(population,N)
    if(!nullHypothesis[i]) treatment <- treatment + delta
    t.test(treatment,control)$p.val 
  })
hist(pvals,breaks=seq(0,1,0.05))
abline(h=m0/20)
```
The first bar on the left represents cases with p-values smaller than 0.05. Note that from the horizontal line we can infer that about 1/2 are false positives. This is in agreement with an FDR of 0.50. If we look at the bar for 0.01 we see can infer a lower FDR, as expected, but would call less features significant.

```{r}
hist(pvals,breaks=seq(0,1,0.01))
abline(h=m0/100)
```


### Benjamini-Hochberg (Advanced)

We want to construct a procedure that guarantees the FDR to be below a certain level $\alpha$. For any given $\alpah$, the Benjamini-Hochberg (1995) is very practicaly because it simply requires we compute p-values for each of the individual tests. With these p-values in hand, we define the procedure.

The idea is to first order the p-values in increasing order: $p_{(1)},\dots,p_{(m)}. Then define $k$ to be the largest $i$ for which

$$p_{(i)} \leq \frac{i}{m}\alpha$$

The procedure is to reject tests with p-values larger than $p_{(k)}$. Here is an example of how we would select the $k$ with code:

```{r}
alpha <- 0.25
N <- 12
m <- 100
nullHypothesis <- c( rep(TRUE,90), rep(FALSE,10))
delta <- 3
pvals <- sapply(1:m, function(i){
    control <- sample(population,N)
    treatment <- sample(population,N)
    if(!nullHypothesis[i]) treatment <- treatment + delta
    t.test(treatment,control)$p.val
  })
min(pvals)
mypar(1,1)
i = seq(along=pvals)
mypar(1,2)
plot(i,sort(pvals))
abline(0,i/m*alpha)
##close-up
plot(i[1:15],sort(pvals)[1:15],main="Close-up")
abline(0,i/m*alpha)
k <- max( which( sort(pvals) < i/m*alpha) )
cutoff <- sort(pvals)[k]
cat("k =",k,"p-value cutoff=",cutoff)
```
We can show, mathematically, that this procedure has FDR lower than 25%. Please see Benjamini-Hochberg (1995) for details. 

Note, we don't have to run the complicated code above as we have functions to do this. For example, using the p-values `pvals` computed above
we simply type the following:
```{r}
fdr <- p.adjust(pvals, method="fdr")
mypar(1,1)
plot(pvals,fdr)
abline(h=0.25,v=cutoff) ##cutoff was computed above
```

We can run a Monte-carlo simulation to confirm that the FDR is in fact lower than .25. Note that we compute all p-values first, and the uses these to decide which get called.

```{r}
alpha <- 0.25
N <- 12
m <- 100
nullHypothesis <- c( rep(TRUE,100), rep(FALSE,0))
delta <- 1
B <- 1000 ## number of simulations
Qs <- replicate(B,{
  ##Compute all p-values first!
  pvals <- sapply(1:m, function(i){
    control <- sample(population,N)
    treatment <- sample(population,N)
    if(!nullHypothesis[i]) treatment <- treatment + delta
    pvals <- t.test(treatment,control)$p.val 
  })
  ## then the FDR
  calls <- p.adjust(pvals,method="fdr") < alpha
  R=sum(calls)
  Q=ifelse(R>0,sum(nullHypothesis & calls)/R,0)
  return(Q)
})
mypar(1,1)
hist(Qs) ##Q is a random variable, this is its distribution
FDR=mean(Qs)
print(FDR)
```

The FDR is lower than 0.25. This is to be expceted because we need to be conservative to assure the FDR $\leq$ 0.25 for any value of $m_0$, for example for the extreme case where every hypothesis tested is null: $m=m_0$. If you re-do the simulation above for this case you will find that the FDR increases. 

Finally, note that the `p.adjust`function has several options for error rate controling procedures:
```{r}
p.adjust.methods
```
It is important to keep in mind that these options differ not just different approaches to estimating error rates but also that different error rates are estimated: namely FWER and FDR. This is important distinction. More information is available from 

```{r,eval=FALSE}
?p.adjust
```

In summary, requiring that  FDR $leq$ 0.25 is a much more lineant requirement FWER $leq$ 0.05. Although we will end up with many more false positives, FDR gives us much more power. This makes it particularly appropriate for discovery phase experiments.

## Direct approach to FDR  and q-values (Advanced)

Here we review the results described by John D. Storey in
J. R. Statist. Soc. B (2002). One major distinction between this approach the Benjamni and Hochberg's is that we are no longer going to set a $\alpha$ level a-priori. Because in many high-throughput we are interested in obtaining some list for validation, we can instead decide before hand that we will consdier all tests with $p-values$ smaller than 0.01. We then want to attach an estimate of an error rate. Using this approach we then we are guaranteed to have $R>0$. Note that in the FDR definition above we assigned $Q=0$ in the case that $R=V=0$. We are therefore computing 

$$
\mbox{FDR} = E\left( \frac{V}{R} \mid R>0\right) \mbox{Pr}(R>0)
$$

In the approach proposed by Storey we are conditioning on $R>0$ and he suggests that we instead compute the _positive FDR_ 

$$
\mbox{pFDR} = E\left( \frac{V}{R} \mid R>0\right) 
$$

A second distinction is that while Benjamin and Hochberg's procedure controls under the worse case scenario in which all null hypotheses are true ($m=m_0$), Storey proposes we actually try to estimate $m_0$ from the data. Because in high-througput experiment we have so much data, this is certainly posible. The general idea is to pick a $\lamba$ and assume that tests obtaining p-values $>\lamba$ are moslty from true null hypothesis. We can then estimate $\pi_0 = m_0/m$ as: 
$$
\hat{\pi}_0 = \frac{\#\left\{p_i > \lambda \right\} }{ (1-\lambda) m }
$$
Here is an example setting $\lamba=0.1$:

```{r}
m <- 10000
m0 <- m*.9 
m1 <- m-m0
nullHypothesis <- c( rep(TRUE,m0), rep(FALSE,m1))
delta <- 3
pvals <- sapply(1:m, function(i){
    control <- sample(population,N)
    treatment <- sample(population,N)
    if(!nullHypothesis[i]) treatment <- treatment + delta
    t.test(treatment,control)$p.val 
  })
hist(pvals,breaks=seq(0,1,0.05),freq=FALSE)
lambda = 0.1
pi0=sum(pvals> lambda) /((1-lambda)*m)
abline(h= pi0)
```

With this estimate in place we can, for example, alter the Benjamini and Hochberg procedure to select the $k$ to be the largest value so that 

$$\hat{\pi}_0 p_{(i)} \leq \frac{i}{m}\alpha$$

However, instead of doing this we compute a _q-value_ for each test. If a feature resulted in a p-value of $p$, the q-value is the estimated pFDR for a list of all the features with a p-value at least as small as $p$.

In R this can be computed with the `qvalue` function in the `qvalue` package:

```{r}
library(qvalue)
res <- qvalue(pvals)
qvals <- res$qvalues
plot(pvals,qvals)
```
we also obtain the estimate of $\hat{\pi}_0$:

```{r}
res$pi0
```


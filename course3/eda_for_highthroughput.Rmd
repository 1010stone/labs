---
layout: page
title: Basic EDA for high-throughput data
---

```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

```{r,message=FALSE}
library(rafalib)
```

# Introduction 
An underappreciated advantage of highthroughput data is that the problems with the data are sometimes more easilty exposed. The fact that we have thousands of measurements permits us to see problems that are not apparent when only a few measurements are available. A powerful way to detect these problems is with highthroughput technologies. Here we review some of the plots that allow us to detect quality problems.

We will use the results of an analysis from a previous section:

```{r}
library(GSE5859Subset)
data(GSE5859Subset)
g <- factor(sampleInfo$group)
results <- rowttests(geneExpression,g)
pvals <- results$p.values

##Null data
m <- nrow(geneExpression)
n <- ncol(geneExpression)
randomData <- matrix(rnorm(n*m),m,n)
nullpvals <- rowttests(randomData,g)$p.values
```

# Volcano plots

As we described in the Introduction chapter, reporting only p-values is a mistake when we can also report effect sizes. With high-throughput data we can visualize the results by making a plot. The idea behind a _volcano plot_ is to show these for all features. In the y-axis we plot -log (base 10) p-values and on the x-axis the effect size. By using - log (base 10) we have the "highly significant" results be high on the plot. Using log permits us to better distinguish between, say, 0.05 and 0.001.  Here is the volcano plot for our results above:

```{r}
plot(results$dm,-log10(results$p.value),xlab="Effect size",ylab="- log (base 10) p-values")
```


# p-value histograms

Another plot we can make to get an overall idea of what our results are telling us is to make histograms of p-values. Note that when we generate completely null data the histrogram follows a uniform distribtuion (we will say more about this). With our original data set we see a higher frequency of smaller p-values

```{r}
mypar2(1,2)
hist(nullpvals,ylim=c(0,1400))
hist(pvals,ylim=c(0,1400))
```



# Data boxplots and histogram

With high throughput data we have thousands of measurements for each experimental unit. This can help us detect quality issues. For example if one sample has a completely different distribution than the rest. This could be due to real biological diffrences, but more often than not it is due to a technical problem. 

```{r,message=FALSE}
library(SpikeIn) 
data(SpikeIn95)
e <- log2(exprs(SpikeIn95))
```

H

A quick way to explore the distribution of several samples is to simply plot boxplots. Here we see that sample 49 is somewhat different from the rest. 

```{r}
library(rafalib)
mypar2(1,1)
boxplot(e,range=0,names=1:ncol(e),col=ifelse(1:ncol(e)==49,1,2))
```

If the number of samples is to large one can simply show the boxplot summaries without (cite Karl Broman):

```{r}
qs <- t(apply(e,2,quantile,prob=c(0.05,0.25,0.5,0.75,0.95)))
matplot(qs,type="l")
```

We can also plot all the histograms. Because we have so much data we can use small bins and smooth the heights of the bars and the plot _smooth histograms_. 

```{r}
mypar2(1,1)
shist(e)
```

# MA plot

Scatterplots and correlation are not the best tools to detect problems. Note for example that  1,2,3,4 and 100,200,300,400 two lists with very different values have perfect correlation. A better measure is the differences between the values and therefore a better plot is a rotation of the scatter plot containg the differences (log ratios) on the y-axis and the averages (in the log scale) on the x-axis. This plot is a refered to as an MA-plot. 


```{r}
x <- geneExpression[,3]
y <- geneExpression[,16]
mypar(1,1)
plot(x,y)
plot((x+y)/2,x-y)
```




op---
title: "Linear Algebra Examples "
author: "Rafa"
date: "January 31, 2015"
output: html_document
layout: page
---

```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

# Introduciton


To compute the sample average and variance of our data we use these formulas $\bar{Y}=\frac{1}{N} Y_i$ and $\mbox{var}(Y)=\frac{1}{N} \sum_{i=1}^N (Y_i - \bar{Y})^2$ with $\bar{Y}$ the average. We can represent these with matrix multiplication. First define this $N \times 1$ matrix made just of 1s

$$
A=\begin{pmatrix}
1\\
1\\
\vdots\\
1
\end{pmatrix}
$$

This implies that (note that we are multiplying by the scalar $1/N$)

$$
\frac{1}{N}
\mathbf{A}^\top Y = \frac{1}{N}
\begin{pmatrix}1&1&,\dots&1\end{pmatrix}
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}
=
\frac{1}{N} \sum_{i=1}^N Y_i
= \bar{Y}
$$

In R we multiply matrix using `%*%`

```{r}
y <- father.son$sheight
print(mean(y))

N <- length(y)
Y<- matrix(y,N,1)
A <- matrix(1,N,1)
barY=t(A)%*%Y / N

print(barY)
```

As we will see later, multiplying the transpose of a matrix with another is very common in statistics. So common there is a function in R

```{r}
bary=crossprod(A,Y) / N
print(barY)
```

For the variance we note that if

$$
\mathbf{r}=\begin{pmatrix}
Y_1 - \bar{Y}\\
\vdots\\ 
Y_N - \bar{Y}
\end{pmatrix}, 
\frac{1}{N} \mathbf{r}^\top\mathbf{r} = 
\frac{1}{N}\sum_{i=1}^N (Y_i - \bar{Y})^2
$$
And in R (note:if you only send one matrix into `crossprod` it computes: $r^\top r$)

```{r}
r <- y - barY
crossprod(r)/N
```

Which is equivalent to 
```{r}
var(y) 
```
except for the fact that the R function `var` divides by $N-1$. 

```{r}
var(y) * (N-1) / N
```

## Linear models

Now we are ready to put all this to use. Let's start with Galton's example. If we define these matrix 

$$
\mathbf{Y} = \begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix},
\mathbf{X} = \begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix},
\mathbf{\beta} = \begin{pmatrix}
\beta_0\\
\beta_1\\
\end{pmatrix} \mbox{ and }
\mathbf{\varepsilon} = \begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$

Then we can write the model 

$$ 
Y_i = \beta_0 + \beta_1 x_i + \varepsilon, i=1,\dots,N 
$$

as 

$$
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}
= 
\begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix}
\begin{pmatrix}
\beta_0\\
\beta_1\\
\end{pmatrix} +
\begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$

or simply: 
$$\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
$$

which is a much simpler way to write it. 

<b>Optional homework</b>: write out the matrices multiplication convince yourself that this this is the case.

The RSS equation becomes simpler as well as it is the following cross-product:

$$
(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^\top
(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})
$$

So now we are ready to determine which values of $\beta$ minimize the above. There are a series of rules that permit us to compute partial derivatives equations in matrix notation. The only one we need here tells us that the derivative of the above equation is:




Note: that the RSS is like a square (multiply something by itself) and that this formula is similar to  similar to the derivative of $f(x)^2$ being $2f(x)f'(x)$. 

By equating the derivative to 0 and solving for the $\beta$ we will have our solution:

$$
2 \mathbf{X}^\top (\mathbf{Y} - \mathbf{X} \boldsymbol{\hat{\beta}})=0
$$

$$
\mathbf{X}^\top \mathbf{X} \boldsymbol{\hat{\beta}} = \mathbf{X}^\top \mathbf{Y}   
$$


$$
\boldsymbol{\hat{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}   
$$

and we have our solution. 
We usually put a hat on the $\beta$ that solves this, $\hat{\beta}$ as it is an estimate of the "real" $\beta$ that generated the data.

Let's see how it works in R

```{r}
library(UsingR)
x=father.son$fheight
y=father.son$sheight
X <- cbind(1,x)
betahat <- solve(t(X)%*%X)%*%t(X)%*%y
###or
betahat <- solve(crossprod(X))%*%crossprod(X,y)
```


Now we can see the results of this by computing the estimated $\hat{\beta}_0+\hat{\beta}_1 x$ for any value of $x$

```{r}
newx <- seq(min(x),max(x),len=100)
X <- cbind(1,newx)
fitted <- X%*%betahat
plot(x,y,xlab="Father's height",ylab="Son's height")
lines(newx,fitted,col=2)
```

This $\hat{\boldsymbol{\beta}}=(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}$
 is one of the most widely used results in data analysis. One of the beauties of this approach is that we can use the same approach for the other problem. Note we are using almost the same exact code:


```{r}
X <- cbind(1,tt,tt^2)
y <- d
betahat <- solve(crossprod(X))%*%crossprod(X,y)
newtt <- seq(min(tt),max(tt),len=100)
X <- cbind(1,newtt,newtt^2)
fitted <- X%*%betahat
plot(tt,y,xlab="Father's height",ylab="Son's height")
lines(newtt,fitted,col=2)
```

Note the resulting estimates are what we expect:

```{r}
betahat
```

The Tower of Pisa is about 56 meters high, there is no initial velocity and half the constant of gravity is 9.8/2=4.9.

### The lm function
R has a very convenient function that fits these models. We will learn more about this function later. But here is a preview

```{r}
X <- cbind(tt,tt^2)
fit=lm(y~X)
summary(fit)
```
















